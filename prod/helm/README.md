# Helm at Toolchain

We use [Helm](https://docs.helm.sh/) for packaging and deploying Kubernetes services.

## Background

In Helm terminology:

- A `chart` is a Helm package, containing definitions of various Kubernetes resources.
  A chart is identified by a name and a semver, defined in a `Chart.yaml` file.
- A `release` is a running instance of a chart and has a name that can be assigned by the
  operator (e.g., `users-prod`), or autogenerated by Helm (e.g., `sinister-lion`).
  The former are more informative, the latter more evocative.
- A `repo` is a collection of published, semver'ed charts, packaged in a `*.tgz` file.
  This allows releases to be deployed and upgraded consistently and repeatably from a given semver of a given chart.

Chart sources live under subdirs of this directory.  Releases are deployed from charts in one of two ways:

- Directly from local source, which is useful for development.

- From published, versioned packages in a repo, which is required for production deploys.

## Setup

To use Helm:

- Install the helm cli. Our [client setup script](../../setup.sh) will do this for you on macOS.
  See [Helm's installation instructions](https://helm.sh/docs/using_helm/#install-helm)) for Linux.

  If you run into client / server version mismatch issues in subsequent commands, you can install the appropriate
  client version using `brew` and the corresponding link from GitHub (more context, including examples, can be
  found in <https://github.com/helm/charts/issues/5239#issuecomment-423357321>).

  This setup script also calls [helm_setup.sh](../../helm_setup.sh) Which does the following:

  - Installs the [helm s3 plugin](https://github.com/hypnoglow/helm-s3) which is used for our private helm repo hosted on s3: s3://us-east-1.toolchainlabs.com/helm/charts
  - Installs the [helm diff plugin](https://github.com/databus23/helm-diff) which supports showing a diff explaining what a helm upgrade would change.
  - Adds our private helm repo and names it `helm-e1`.<sup>1</sup>
  - Adds the third party helm repo `jetstack` (for the cert-manager chart).
  - Adds the standard third party `stable` helm repo.
  - Adds the AWS EKS [helm charts repo](https://github.com/aws/eks-charts).
  - Removes the local helm repo, which we don't use, and will cause spurious error messages
- Create your user namespace on the dev cluster:
  - `prod/kubernetes/kubectl_setup.sh dev-e1-1 && src/sh/kubernetes/create_namespace.sh $USER`

Note that by default the Helm CLI will talk to whichever Kubernetes cluster your `kubectl` is
set up to talk to. Your underlying AWS CLI's default region must be the one in which that cluster resides.

## Deploying a release directly from source

### Using a convenience script

```shell
./prod/helm/install_dev.sh <service_names>
```

See [here](../../src/python/toolchain/service/SERVICE_NAMES.md) for info about service names.
This will build the service's gunicorn Docker images and push them to AWS ECR and install them on the Kubernetes dev cluster. So is useful for iterating during development.
The script builds generates pex files for specified services, builds the docker images around them, publishes the docker images to AWS ECR and then deploys those services to your Kubernetes namespace (in the dev-e1-1 cluster).

Service groups names can also be used see [here](../../src/python/toolchain/service/SERVICE_NAMES.md#support-for-service-groups) for more information.

### Connecting to your dev service

You can only connect to dev services on Kubernetes via a local port-forward.

Use `./src/sh/kubernetes/port_forward.sh <service_name>` which should give you output similar to the following:

```shell
Running kubectl port-forward service/infosite 9000:80
Access service at http://localhost:9000
Forwarding from 127.0.0.1:9000 -> 8000
Forwarding from [::1]:9000 -> 8000
```

Visit `localhost:9000` (or whichever port your output mentions) to view your dev service.
See [here](../../src/python/toolchain/config/DEV_PORTS.md) for details.
[Kube Forwarder](https://kube-forwarder.pixelpoint.io/) is a helpful GUI tool that makes it easy to manage port forwarding from Kubernetes clusters.

## Installing charts to production/staging

For our production/staging environments, we require that charts will be published to our helm chart repo (helm-e1) in order to deploy them to our production Kubernetes cluster (prod-e1-1).

For the most common deploys, [service groups](../../src/python/toolchain/service/SERVICE_NAMES.md#support-for-service-groups) should be utilized.

Build and deploy script will check your local git status to make sure we only deploy merged changes to our production and staging environments.

There are a few scripts that are supported:

- `prod/helm/build_and_install_prod.sh` - This script does the following:
  - Build PEX files for specified services
  - Build Docker images for specified service and publish them to ECR
  - Update helm chart's values.yaml files with the just built gunicorn docker image.
      **Note**: This changes files under source control, the change needs to be committed and PR'ed.
  - Publish the chart to our chart repo (only if a chart version change is detected).
      **Note** that updating the values file doesn't require the chart version to be updated.
  - Install the the chart to the prod cluster (staging namespace by default).

- `prod/helm/install_prod.sh` - This script does the following:
  - Package and update the helm chart in our repo (helm-e1) - only if the local chart version is doesn't match the chart version in the repo
  - Install the the chart to the prod cluster (staging namespace by default).

## Edge deploy

For more extensive testing in a production environment we have an edge environment (which means, an edge namespace on the kubernetes cluster). Currently this is used only in the Remoting production cluster.

The purpose of this enviroment is to faciliate long term testing of changes that can not be done in the dev enviroment (for example, because CI systems can't hit our dev enviroment) but on the other hand those changes might have negative impact on existing customers so we don't want them in the regular production environment either.

Deploying to the edge enviroment (currently only supported on the remoting cluster) is done using the same scripts used to deploy to staging and prod (`build_and_install_prod.sh` and `install_prod.sh`) by passing them the `--edge` argument.
Note that when deploying using `build_and_install_prod.sh` the script will modify the values.yaml files of the charts being deployed. Those changes should be discarded as the versions we want to have in the values.yaml files are the versions we have in our regular prod environment/namespace.

## Failing end 2 end tests in staging when provisioning new load balancers

We have certain services that we don't keep running in the staging env (infosite, webhooks, toolshed) this means that when those services are deployed to staging a new AWS EC2 Load Balancer need to be provisioned (it happens automatically) and a DNS record also needs to be provisioned (also happens automatically).
But both of those operations can take some time (especially the DNS record creation, it is created quickly but takes a few minutes to propagate).

This means that helm end 2 end tests that are executed as part of the deploy (currently webhooks & infosite have those) will likely fail.
The test failures do not break or stop the deploy process. There is logic in those tests that tries to [wait for the DNS to propagate](../../src/python/toolchain/prod/e2e_tests/helpers.py), but it is not working properly for this use case at this time.
The workaround is to let the deploy finish and when the site is up (for infosite, access it via browser and for webhooks, use curl)
and run the tests again using the helm test command. For example: `helm test staging-infosite` or `helm test staging-webhooks`

## Rollback a bad deploy

If we deployed a bad version, the quickest way to get to the previous working version is using helm rollback.

This step should be followed either by fixing the root issue or rolling back the change in the repo and deploying a new version.

To rollback a helm release use the [helm rollback command](https://helm.sh/docs/helm/helm_rollback/). This command takes the release name as a parameter, which is `<namespace>-<service name>`, e.g. `prod-buildsense-api`. To see a list of releases use the [helm list command](https://helm.sh/docs/helm/helm_list/#helm) with the namespace param, for example: `helm list -n prod`. So, to revert a bad servicerouter deploy in production do: `helm rollback prod-servicerouter`.

## Revert a production deploy

If the deployed version is bad, and we find out after we deploy, we need to revert to the previous version.

- [Revert the PR](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/reverting-a-pull-request) that included the related helm chart updates (values.yaml files)
- Merge the revert PR
- From a clean branch (that includes the reverted PR) run the `prod/helm/install_prod.sh` script passing all the services that needs to be reverted.

## InfoSite special flow

For all of our services we keep the staging version running since there is no downside to doing that.
For infosite, we don't keep the staging version up beyond the time needed to test new release, since it is publicly available, and we don't want search engine to index it instead of our prod site.
So after finishing the release process and deploying a new version of info site to production, we remove the infosite release from staging:

```bash
helm delete staging-infosite
```

## Miscellaneous

You can also run `helm` cli commands directly as needed.

E.g., to delete a release (be careful not to delete prod releases unless you know what you're doing):

```helm delete benjy-buildsense-api```

See the [Helm docs](https://docs.helm.sh/) for details on the various commands.

Note that all `helm` commands will default to the namespace that `kubectl` defaults to.
If you set up `kubectl` [as recommended](../kubernetes/README.md) this will be your dev namespace.
You can select a different namespace with the `--namespace` flag

<sup>1</sup> For reference, the repo was created thus:

```helm s3 init s3://helm.us-east-1.toolchain.com/charts```

## Deploy Checklist

1. Make sure your change is merged with master and you are at HEAD of master with no local changes.
2. Deploy your change to dev and make sure everything is working. `./prod/helm/install_dev.sh <service_name>` to install it on the dev cluster. `./src/sh/kubernetes/port_forward.sh <service_name>` to make it viewable with `localhost:9000` (or whatever port it prints in the output.)
3. Providing everything looks good get the prod context set up locally. `kubectl config get-contexts` to see what is set up locally already. This should show you only `dev-e1-1`. Run `./prod/kubernetes/kubectl_setup.sh prod-e1-1` to set up the prod cluster. You may also want to run `kubectl config set-context --current --namespace=staging` to prevent accidentally making changes to the prod namespace.
4. Run `./prod/helm/build_and_install_prod.sh <service_name>` to build the image and install it in staging.
   Running this script, will check that you your git state is valid for deploy:
    - No local changes (staged or unstaged)
    - The last commit in the current local branch is a master commit (i.e. not a local unmerged branch)
5. Once deploy to staging is done, the script would have modified helm chart values.yaml files for the deployed service(s). Commit those changes and submit a PR with them. The PR title should be something along the lines of "Update `<list of services, or just services>` in prod/staging" so it easy to track down commits that updated versions of the code in prod by looking at the commit log.

6. Test relevant changes in the staging environment.

7. Merge the PR created in step 5 and switch to a branch that contains the merged PR without any local changes.

8. Run `./prod/helm/install_prod.sh <service_name>`
9. Have a good look at your changes again (in production)
10. Clean up your local setup to make it impossible to accidentally edit prod: `kubectl config delete-context prod-e1-1`

11. If you were deploying infosite, remove it from staging so google doesn't index it: `helm delete -n staging staging-infosite`
    - Congratulate yourself, you're done and everything still works!

### Notes

- For Toolshed, Webhooks (and Infosite, mentioned earler) - we don't keep the staging versions around after deploy is done.
  So after deploy is done, those releases need to be deleted (if they were part of the deploy) using the `helm delete` command.
  For example: `helm delete staging-toolshed`

- Workflow services (worker & maintenance charts) should not be deployed to the prod namespace.
  This is because we don't have a good way to isolated work units between those environments.
  Case in point: if there is a new type of work unit payload, when that version is deployed to staging (and we create a work unit) the prod instance of the workflow service might try to pick it up in order to process it, since it doesn't have the new code yet (workers, work unit payload) it will blow up.
  The solution for this is to make to workflow service env aware and have workers only pick up work from their own envirnment, which basically boils down to adding an environment column to the WorkUnit model have have a workers filter work units by that env column.
